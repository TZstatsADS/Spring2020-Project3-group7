---
title: "LDA-KNN GBM Adaboost & Random Forest"
output:
  html_document:
    df_print: paged
---

```{r}
## Load package
library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
```

```{r}
### Step 0 set work directories
set.seed(0)
setwd("C:/Users/YWJ97/Desktop/ADS/Spring2020-Project3-group7/Spring2020-Project3-Group7/data/train_set/")

train_dir <- "C:/Users/YWJ97/Desktop/ADS/Spring2020-Project3-group7/Spring2020-Project3-Group7/data/train_set/" # This will be modified for different data sets.

train_image_dir <- paste(train_dir, "images/", sep="")

train_pt_dir <- paste(train_dir,  "points/", sep="")

train_label_path <- paste(train_dir, "label.csv", sep="") 
```

```{r}
### Step 1: set up controls for evaluation experiments.

run.cv=TRUE # run cross-validation on the training set

K <- 5  # number of CV folds

run.feature.train=TRUE # process features for training set

run.test=TRUE # run evaluation on an independent test set

run.feature.test=TRUE # process features for test set
```

```{r}
### Step 2: import data and train-test split 
#train-test split

info <- read.csv(train_label_path)

n <- nrow(info)

n_train <- round(n*(4/5), 0)

train_idx <- sample(info$Index, n_train, replace = F)

test_idx <- setdiff(info$Index,train_idx)

n_files <- length(list.files(train_image_dir))

```

```{r}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
key_point=c(1:9,19:25,35:44,64:71)

readMat.matrix <- function(index){
  
     return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
  
}

#load fiducial points

fiducial_pt_list<- lapply(1:n_files, readMat.matrix)

##save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")

leftmid_idx <- c(1:9,19:26,35:44,50:52,56:59,62,63,64:71)

length(leftmid_idx)

fiducial_pt_list_lm <- lapply(fiducial_pt_list, function(mat){return(mat[leftmid_idx,])})

```

```{r}

#306 duplicate index

n.m <- 44

## index within leftmid_idx

middle <- c(18:21,27,30,31,34,35,44)

length(middle)

# 946=((44*44)-44)/2

d <- rep(1, 946)

m=matrix(rep(0, n.m^2),n.m,n.m,byrow=T)

k=1

for(i in 1:(n.m-1))for(j in (i+1):n.m){
  
  m[i,j] <- m[j,i] <- d[k]
  
  k=k+1
  
}
# mark distance related to 10 middle point
m[middle,] <- -1

m[,middle] <- -1

m[middle,middle] <- -2

ind <- c()

k=1

for(i in 1:(n.m-1)) {
  
  for(j in (i+1):n.m){
  
    if(m[j,i] == -1) ind <- c(ind, k)
  
    k=k+1
  
  }
}

## mark duplicate horizontal distance

dup_horiz <- ind[!(ind %in% cumsum(43:1)[-middle])]

```

```{r}
### Step 3: construct features and responses

source("C:/Users/YWJ97/Desktop/ADS/Spring2020-Project3-group7/Spring2020-Project3-Group7/lib/feature.R")

dat_train_leftmid<-feature(fiducial_pt_list_lm,train_idx)

dat_train_leftmid<-dat_train_leftmid[,-dup_horiz]

dat_test_leftmid<-feature(fiducial_pt_list_lm,test_idx)

dat_test_leftmid<-dat_test_leftmid[,-dup_horiz]

  
library(dplyr)

library(MASS)

a<-lda(dat_train_leftmid[,-1587],grouping=dat_train_leftmid$emotion_idx)

score_lda<-as.matrix(dat_train_leftmid[,-1587]) %*% a$scaling #new 21 variables

test_score_lda<-as.matrix(dat_test_leftmid[,-1587]) %*% a$scaling

lda_data=rbind(score_lda,test_score_lda)

lda_class=c(dat_train_leftmid[,1587],dat_test_leftmid[,1587])



## Now we get data with reduced feature. We can proceed to CLASSIFICATION
```

```{r}
###############################################################################################
######################################---------KNN-------######################################
###############################################################################################

### Step 4.1: Train KNN model with training features and responses

# knn_1=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=1)
# accuray_1=sum(knn_1==dat_test_leftmid[,1587])/500
# 
# knn_3=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=3)
# accuray_3=sum(knn_3==dat_test_leftmid[,1587])/500
# 
# knn_5=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=5)
# accuray_5=sum(knn_5==dat_test_leftmid[,1587])/500
# 
# knn_7=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=7)
# accuray_7=sum(knn_7==dat_test_leftmid[,1587])/500
# 
# knn_9=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=9)
# accuray_9=sum(knn_9==dat_test_leftmid[,1587])/500
# 
# knn_21=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=21)
# accuray_21=sum(knn_21==dat_test_leftmid[,1587])/500
# 
# knn_31=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=31)
# accuray_31=sum(knn_31==dat_test_leftmid[,1587])/500
# 
# knn_51=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=51)
# accuray_51=sum(knn_51==dat_test_leftmid[,1587])/500
# 
# knn_81=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=81)
# accuray_81=sum(knn_81==dat_test_leftmid[,1587])/500
# 
# knn_121=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=121)
# accuray_121=sum(knn_121==dat_test_leftmid[,1587])/500
# 
# knn_151=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=151)
# accuray_151=sum(knn_151==dat_test_leftmid[,1587])/500
# 
# knn_201=knn3Train(score_lda,test=test_score_lda,cl=dat_train_leftmid[,1587],k=201)
# accuray_201=sum(knn_201==dat_test_leftmid[,1587])/500

## Cross Validation
tuning_p=c(1,3,5,7,9,21,31,41,51,61,71,81,121,151,201)

n_p=length(tuning_p)

KNN_result=matrix(0,K,n_p)

cv.k=createFolds(1:2500,K)

for (j in 1:K){
  
  dat_test_cv=lda_data[cv.k[[j]],]
  
  dat_train_cv=lda_data[-cv.k[[j]],]
  
  dat_test_class=lda_class[cv.k[[j]]]
  
  dat_train_class=lda_class[-cv.k[[j]]]
  
  for (i in 1:n_p){
    
    model=knn3Train(dat_train_cv,test=dat_test_cv,cl=dat_train_class,k=tuning_p[i])
    
    KNN_result[j,i]=sum(model==dat_test_class)/500
    
  }
  
}

KNN_result=as.data.frame(KNN_result)

names(KNN_result)=paste("K=",tuning_p)

colMeans(KNN_result)

# From the table we can see the best K is 71, accuracy = 0.7516

Best_K=71 

```


```{r}
###############################################################################################
######################################---------GBM-------######################################
###############################################################################################

### Step 4.2: Train GBM model with training features and responses

#install.packages("gbm")

library(gbm)
# # 加载包和数据
# install.packages("gbm")
# install.packages("mlbench")
# library(gbm)
# library(mlbench)
# data(PimaIndiansDiabetes2,package='mlbench')
# # 将响应变量转为0-1格式
# data <- PimaIndiansDiabetes2
# data$diabetes <- as.numeric(data$diabetes)
# data <- transform(data,diabetes=diabetes-1)
# # 使用gbm函数建模
# model <- gbm(diabetes~.,data=data,shrinkage=0.01,
#              distribution='bernoulli',cv.folds=5,
#              n.trees=3000,verbose=F)
# # 用交叉检验确定最佳迭代次数
# best.iter <- gbm.perf(model,method='cv')
# best.iter
# summary(model,best.iter)
# plot.gbm(model,1,best.iter)
# # 用caret包观察预测精度
# library(caret)
# data <- PimaIndiansDiabetes2
# fitControl <- trainControl(method = "cv", number = 5,returnResamp = "all")
# model2 <- train(diabetes~., data=data,method='gbm',distribution='bernoulli',trControl = fitControl,verbose=F,tuneGrid = data.frame(.n.trees=best.iter,.shrinkage=0.01,.interaction.depth=1))
# model2
lda_data_class=cbind(lda_data,lda_class)
lda_data_class=as.data.frame(lda_data_class)
names(lda_data_class)
model_gbm=gbm(lda_class~.,data=lda_data_class,shrinkage = 0.01,distribution = 'multinomial',cv.folds =5, n.trees=3000, verbose = F)
best.iter <- gbm.perf(model_gbm,method='cv')
summary(model_gbm,best.iter)

#extract 10 random 500 test case
gbm_result=rep(0,10)
for (i in 1:10){
  index=sample(1:2500,500)
  test=lda_data_class[index,]
  pred_gbm=predict.gbm(model_gbm,test,n.trees=1948, type="response")
  gbm_result[i]=sum(apply(pred_gbm,1,which.max)==test[,22])/500
}
mean(gbm_result)
##0.9034
```

```{r}
###############################################################################################
###################################---------Adaboost-------####################################
###############################################################################################
### Step 4.3: Train Adaboost model with training features and responses




```

```{r}
###############################################################################################
###############################---------Random Forest-------###################################
###############################################################################################
### Step 4.1: Train KNN model with training features and responses




```


```{r}
###############################################################################################
####################################---------SVM-------########################################
###############################################################################################
### Step 4.1: Train KNN model with training features and responses




```

