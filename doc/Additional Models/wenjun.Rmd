---
title: "LDA-KNN GBM Adaboost SVM RF"
author: "Wenjun Yang wy2347"
date: "3/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, echo=F}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("ggplot2")){
  install.packages("ggplot2")
}

if(!require("caret")){
  install.packages("caret")
}

if(!require("MASS")){
  install.packages("MASS")
}

if(!require("parallel")){
  install.packages("parallel")
}

if(!require("data.table")){
  install.packages("data.table")
}

if(!require("gbm")){
  install.packages("gbm")
}

if(!require("e1071")){
  install.packages("e1071")
}
if(!require("xgboost")){
  install.packages("xgboost")
}

if(!require("caret")){
  install.packages("caret")
}
if(!require("caTools")){
  install.packages("caTools")
}
if(!require("kernlab")){
  install.packages("kernlab")
}
if(!require("Matrix")){
  install.packages("Matrix")
}
if(!require("adabag")){
  install.packages("adabag")
}
if(!require("tidyverse")){
  install.packages("tidyvesrse")
}


library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(MASS)
library(data.table)
library(parallel)
library(gbm)
library(e1071)
library(xgboost)
library(caret)
library(caTools)
library(kernlab)
library(Matrix)
library(adabag)
library(tidyverse)
```



```{r exp_setup}
setwd("C:/Users/YWJ97/Desktop/ADS/Spring2020-Project3-group7/Spring2020-Project3-Group7/doc")
getwd()
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")

run.cv=FALSE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
```

## **Step 2: import data and train-test split**

We splitted the data to 2000 (80%) for training and 50 (20%) for test.

```{r}
#train-test split
set.seed(10)
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index,train_idx)
```


```{r}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
n_files <- length(list.files(train_image_dir))
readMat.matrix <- function(index){
     return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}

#load fiducial points
fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
```

```{r}
load("../output/fiducial_pt_list.RData")
```


## **Step 3: baseline model**

### Step 3.1: feature construction

This step is converting 78 fiducial points to distances as 6006 features (3003 horizontal distances and 3003 vertical distances).

The time for training and test feature construction are as below (about 1.5s and 0.1s):

```{r}
if(run.feature.train){
  
  source("../lib/feature.R")
  base.feature.construction.start = proc.time()
  tm_feature_train <- NA
  dat_train_base <- feature(fiducial_pt_list, train_idx)
  base.feature.construction.train.end = proc.time()
  
  tm_feature_test <- NA
  dat_test_base <- feature(fiducial_pt_list, test_idx)
  base.feature.construction.test.end = proc.time()
  
  #time for training feature construction
  print(base.feature.construction.train.end - base.feature.construction.start)
  #time for test feature construction
  print(base.feature.construction.test.end - base.feature.construction.train.end)
  
  save(dat_train_base, file="../output/feature_train_base.RData")
  save(dat_test_base, file="../output/feature_test_base.RData")

}
```


### Step 3.2: load feature

```{r}
load("../output/feature_train_base.Rdata")
load("../output/feature_test_base.Rdata")
```

### Step 3.3: baseline model: gradient boosting machine

We use gradient boosting machine with stumps for our baseline model. The training dataset is $2000\times6006$ features and an emotion index list with length 2000 of 22 types as response. The time to train the baseline model is as below (about 307s):

# 我们将带有树桩的梯度提升机gbm用于我们的基线模型。 训练数据集是$ 2000 \ times6006 $个特征和一个情感索引列表，长度为2000的22种类型作为响应。 训练基线模型的时间如下（大约307s）

#训练机train

```{r}
#gbm classifier
base.train.model.start = proc.time()
baseline=gbm(emotion_idx~. ,data =dat_train_base ,distribution = "multinomial",n.trees = 100,
             shrinkage = 0.02,n.minobsinnode = 15,cv.folds = 5)
base.train.model.end = proc.time()
#time for training the baseline model
print(base.train.model.end - base.train.model.start)
```

This is our prediction part for gradient boosting model. The test dataset has the same variables as training data but with only 500 samples. It takes around 9.6s to predict and the test results are as below. The testing accuracy is 43%.

# 这是我们对梯度增强模型的预测部分。 测试数据集具有与训练数据相同的变量，但只有500个样本。 大约需要9.6s进行预测，测试结果如下。 测试精度为43％。

#测试集test

```{r}
#predict on test data
base.test.start = proc.time()
baseline.pred = predict.gbm(object = baseline,
                   newdata = dat_test_base,
                   n.trees = 100,
                   type = "response")
base.test.end = proc.time()
#time for testing the baseline model
print(base.test.end - base.test.start)
#prediction result
baseline.labels = colnames(baseline.pred)[apply(baseline.pred, 1, which.max)]
baseline.cm = confusionMatrix(dat_test_base$emotion_idx, as.factor(as.numeric(baseline.labels)))
print(baseline.cm$byClass[1])
print(baseline.cm$table)
```

### **Step 4: our improved model**

### Step 4.1: construct features and responses
```{r}

feature.construction.start = proc.time()
#delete the duplicated right face
leftmid_idx <- c(1:9,19:26,35:44,50:52,56:59,62,63,64:71)
fiducial_pt_list_lm <- lapply(fiducial_pt_list, function(mat){return(mat[leftmid_idx,])})

#306 duplicates index
n.m <- 44
middle <- c(18:21,27,30,31,34,35,44)
d <- rep(1, 946)
m=matrix(rep(0, n.m^2),n.m,n.m,byrow=T)
k=1
for(i in 1:(n.m-1))for(j in (i+1):n.m){
  m[i,j] <- m[j,i] <- d[k]
  k=k+1
}
m

m[middle,] <- -1
m[,middle] <- -1
m[middle,middle] <- -2
m
ind <- c()
k=1
for(i in 1:(n.m-1)) for(j in (i+1):n.m){
  if(m[j,i] == -1) ind <- c(ind, k)
  k=k+1
}
#关于k的组合，这么多行
#中间含有的所有的k的东西，就是中间的值
ind
load("new_index(1).RData")
dup_horiz <- new_index
dup_horiz=as.numeric(dup_horiz)
dup_horiz
```


```{r feature}
if(run.feature.train){

  source("../lib/feature.R")
  tm_feature_train <- NA
  dat_train <- feature(fiducial_pt_list_lm, train_idx)
  feature.construction.train.end = proc.time()
  
  dat_train <- dat_train[,-dup_horiz]
  feature.construction.train.end = proc.time()
  
  tm_feature_test <- NA
  dat_test <- feature(fiducial_pt_list_lm, test_idx)
  feature.construction.test.end = proc.time()
  
  dat_test <- dat_test[,-dup_horiz]
  feature.construction.test.end = proc.time()
  
  #time for training feature construction
  print(feature.construction.train.end - feature.construction.start)
  #time for test feature construction
  print(feature.construction.test.end - feature.construction.train.end)
  
  save(dat_train, file="../output/feature_train.RData")
  save(dat_test, file="../output/feature_test.RData")
}
```

###  load features

```{r}
load("../output/feature_train.RData")
load("../output/feature_test.RData")
```

### Dimension Reduction
```{r}
a<-lda(dat_train[,-1514],grouping=dat_train$emotion_idx)

score_lda<-as.matrix(dat_train[,-1514]) %*% a$scaling #new 21 variables

test_score_lda<-as.matrix(dat_test[,-1514]) %*% a$scaling

lda_data=score_lda

lda_class=as.factor(dat_train[,1514])

lda_test_data=test_score_lda

lda_test_class=as.factor(dat_test[,1514])

lda_data_class=cbind(lda_data,lda_class)

lda_data_class=as.data.frame(lda_data_class)

lda_data_class=lda_data_class%>%
  
  mutate(lda_class=as.factor(lda_class))

```
### Now we get data with reduced feature. We can proceed to CLASSIFICATION

### KNN
```{r}
###############################################################################################
######################################---------KNN-------######################################
###############################################################################################

### Step 4.1: Train KNN model with training features and responses

## Cross Validation

base.train.model.start = proc.time()

tuning_p=c(1,3,5,7,9,21,31,41,51,61,71,81,121,151,201)

n_p=length(tuning_p)

KNN_result=matrix(0,K,n_p)

set.seed(2020)

cv.k=createFolds(1:2000,K)

for (j in 1:K){
  
  dat_valid_cv=lda_data[cv.k[[j]],]
  
  dat_train_cv=lda_data[-cv.k[[j]],]
  
  dat_valid_class=lda_class[cv.k[[j]]]
  
  dat_train_class=lda_class[-cv.k[[j]]]
  
  for (i in 1:n_p){
    
    model=knn3Train(dat_train_cv,test=dat_valid_cv,cl=dat_train_class,k=tuning_p[i])
    
    KNN_result[j,i]=sum(model==dat_valid_class)/500
    
  }
  
}

KNN_result=as.data.frame(KNN_result)

names(KNN_result)=paste("K=",tuning_p)

colMeans(KNN_result)

# From the table we can see the best K is 61

Best_K=61

base.train.model.end = proc.time()

print(base.train.model.end - base.train.model.start)

base.test.model.start = proc.time()

knn_best=knn3Train(lda_data,test=lda_test_data,cl=lda_class,k=61)

base.test.model.end = proc.time()

print(base.test.model.end - base.test.model.start)

test_accu=sum(knn_best==lda_test_class)/500

test_accu

# accuracy 0.422

# Train: 2.76 Test: 0.06
```


```{r}
###############################################################################################
######################################---------GBM-------######################################
###############################################################################################

### Step 4.2: Train GBM model with training features and responses

#install.packages("gbm")

library(gbm)

base.train.model.start = proc.time()

model_gbm=gbm(lda_class~.,data=lda_data_class,shrinkage = 0.01,distribution = 'multinomial',cv.folds =5, n.trees=3000, verbose = F)

base.train.model.end = proc.time()

print(base.train.model.end - base.train.model.start)

best.iter <- gbm.perf(model_gbm,method='cv')

# best iteration = 1793

summary(model_gbm,best.iter)

base.test.model.start = proc.time()

pred_gbm=predict.gbm(model_gbm,as.data.frame(lda_test_data),n.trees=1975, type="response")

base.test.model.end = proc.time()

print(base.test.model.end - base.test.model.start)

gbm_accu=sum(apply(pred_gbm,1,which.max)==lda_test_class)/500

gbm_accu



## 0.388
## Train: 485.12  Test:0.38

```



```{r}
###############################################################################################
###################################---------Adaboost-------####################################
###############################################################################################
### Step 4.3: Train Adaboost model with training features and responses


base.train.model.start = proc.time()

lda_data_class=lda_data_class%>%

    mutate(lda_class=as.factor(lda_class))


ada=boosting(lda_class~.,lda_data_class,boos=T,mfinal=5)

base.train.model.end = proc.time()

print(base.train.model.end - base.train.model.start)

base.test.model.start = proc.time()

pred_ada=predict.boosting(ada,as.data.frame(lda_test_data),newmfinal = 3)

base.test.model.end = proc.time()

print(base.test.model.end - base.test.model.start)

ada_accu=sum(pred_ada$class==lda_test_class)/500

ada_accu

## The accuray of Adaboost is 0.284

## Train: 4.16  Test: 0.05

```

```{r}
###############################################################################################
####################################---------SVM-------########################################
###############################################################################################

### Step 4.5: Train SVM model with training features and responses

library(e1071)

base.train.model.start = proc.time()

model_svm <- svm(lda_class~., data = lda_data_class, kernal = "radial", cost = 1)

base.train.model.end = proc.time()

print(base.train.model.end - base.train.model.start)

base.test.model.start = proc.time()

test.svm.pred <- as.numeric(predict(model_svm, lda_test_data))

base.test.model.end = proc.time()

print(base.test.model.end - base.test.model.start)

svm_accu=mean(test.svm.pred == lda_test_class)

svm_accu

# Accuracy 0.406

# Train: 1.34    Test: 0.14
```

```{r}
###############################################################################################
##################################---------Logistic-------#####################################
###############################################################################################
### Step 4.6: Train Logistic model with training features and responses

#install.packages("mlogit")

library(nnet)

base.train.model.start = proc.time()

model_logi=nnet::multinom(lda_class~.,lda_data_class)

base.train.model.end = proc.time()

print(base.train.model.end - base.train.model.start)

base.test.model.start = proc.time()

logi_accu=sum(predict(model_logi,lda_test_data)==lda_test_class)/500

base.test.model.end = proc.time()

print(base.test.model.end - base.test.model.start)

logi_accu


## The accuracy of Logistics model is 0.422

## Train 1.90    Test: 0.03
```


```{r}
###############################################################################################
###############################---------Random Forest-------###################################
###############################################################################################

### Step 4.7: Train RF model with training features and responses

#install.packages("randomForest")

library(randomForest)

base.train.model.start = proc.time()

model_rf=randomForest(lda_class~.,lda_data_class,)

base.train.model.end = proc.time()

print(base.train.model.end - base.train.model.start)

base.test.model.start = proc.time()

pred_rf=predict(model_rf,lda_test_data,type="response")

base.test.model.end = proc.time()

print(base.test.model.end - base.test.model.start)

rf_accu=sum(pred_rf==lda_test_class)/500

rf_accu

## The accuracy of random forest is 0.43;
## Train 2.6 Test: 0.07

```


