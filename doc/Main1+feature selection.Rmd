---
title: "Facial Emotion Recognition by PCA and LDA"
author: "Group 7"
date: "3/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, echo=F}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("ggplot2")){
  install.packages("ggplot2")
}

if(!require("caret")){
  install.packages("caret")
}

if(!require("MASS")){
  install.packages("MASS")
}

if(!require("parallel")){
  install.packages("parallel")
}

if(!require("data.table")){
  install.packages("data.table")
}

if(!require("gbm")){
  install.packages("gbm")
}

if(!require("e1071")){
  install.packages("e1071")
}
if(!require("xgboost")){
  install.packages("xgboost")
}

if(!require("caret")){
  install.packages("caret")
}
if(!require("caTools")){
  install.packages("caTools")
}
if(!require("kernlab")){
  install.packages("kernlab")
}
if(!require("Matrix")){
  install.packages("Matrix")
}
if(!require("mlr")){
  install.packages("mlr")
}
if(!require("randomForest")){
  install.packages("randomForest")
}
if(!require("purrr")){
  install.packages("purrr")
}

library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(MASS)
library(data.table)
library(parallel)
library(gbm)
library(e1071)
library(xgboost)
library(caret)
library(caTools)
library(kernlab)
library(Matrix)
library(mlr)
library(randomForest)
library(purrr)
```



```{r exp_setup}
setwd("~/Documents/GitHub/Spring2020-Project3-ads-spring2020-project3-group7/doc")
train_dir <- "../data/train_set/"
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="")

run.cv=FALSE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
```

## **Step 2: import data and train-test split**

We splitted the data to 2000 (80%) for training and 50 (20%) for test.

```{r}
#train-test split
set.seed(10)
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index,train_idx)
```


```{r}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
n_files <- length(list.files(train_image_dir))
readMat.matrix <- function(index){
     return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}

#load fiducial points
fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
```

```{r}
load("../output/fiducial_pt_list.RData")
```


## **Step 3: baseline model**

### Step 3.1: feature construction

This step is converting 78 fiducial points to distances as 6006 features (3003 horizontal distances and 3003 vertical distances).

The time for training and test feature construction are as below (about 1.5s and 0.1s):

```{r}
if(run.feature.train){
  
  source("../lib/feature.R")
  base.feature.construction.start = proc.time()
  tm_feature_train <- NA
  dat_train_base <- feature(fiducial_pt_list, train_idx)
  base.feature.construction.train.end = proc.time()
  
  tm_feature_test <- NA
  dat_test_base <- feature(fiducial_pt_list, test_idx)
  base.feature.construction.test.end = proc.time()
  
  #time for training feature construction
  print(base.feature.construction.train.end - base.feature.construction.start)
  #time for test feature construction
  print(base.feature.construction.test.end - base.feature.construction.train.end)
  
  save(dat_train_base, file="../output/feature_train_base.RData")
  save(dat_test_base, file="../output/feature_test_base.RData")

}
```


### Step 3.2: load feature

```{r}
load("../output/feature_train_base.Rdata")
load("../output/feature_test_base.Rdata")
```

### Step 3.3: baseline model: gradient boosting machine

We use gradient boosting machine with stumps for our baseline model. The training dataset is $2000\times6006$ features and an emotion index list with length 2000 of 22 types as response. The time to train the baseline model is as below (about 307s):

# 我们将带有树桩的梯度提升机gbm用于我们的基线模型。 训练数据集是$ 2000 \ times6006 $个特征和一个情感索引列表，长度为2000的22种类型作为响应。 训练基线模型的时间如下（大约307s）

#训练机train

```{r}
#gbm classifier
base.train.model.start = proc.time()
baseline=gbm(emotion_idx~. ,data =dat_train_base ,distribution = "multinomial",n.trees = 100,
             shrinkage = 0.02,n.minobsinnode = 15,cv.folds = 5)
base.train.model.end = proc.time()
#time for training the baseline model
print(base.train.model.end - base.train.model.start)
```

This is our prediction part for gradient boosting model. The test dataset has the same variables as training data but with only 500 samples. It takes around 9.6s to predict and the test results are as below. The testing accuracy is 43%.

# 这是我们对梯度增强模型的预测部分。 测试数据集具有与训练数据相同的变量，但只有500个样本。 大约需要9.6s进行预测，测试结果如下。 测试精度为43％。

#测试集test

```{r}
#predict on training data
baseline.pred.train = predict.gbm(object = baseline,
                   newdata = dat_train_base,
                   n.trees = 100,
                   type = "response")
#prediction result
baseline.labels.train = colnames(baseline.pred.train)[apply(baseline.pred.train, 1, which.max)]
baseline.cm.train = confusionMatrix(dat_train_base$emotion_idx, as.factor(as.numeric(baseline.labels.train)))
print(baseline.cm.train$byClass[1])

#predict on test data
base.test.start = proc.time()
baseline.pred = predict.gbm(object = baseline,
                   newdata = dat_test_base,
                   n.trees = 100,
                   type = "response")
base.test.end = proc.time()
#time for testing the baseline model
print(base.test.end - base.test.start)
#prediction result
baseline.labels = colnames(baseline.pred)[apply(baseline.pred, 1, which.max)]
baseline.cm = confusionMatrix(dat_test_base$emotion_idx, as.factor(as.numeric(baseline.labels)))
print(baseline.cm$byClass[1])
print(baseline.cm$table)
```

### **Step 4: our improved model**

### Step 4.1: construct features and responses
```{r}

feature.construction.start = proc.time()
#delete the duplicated right face
#choose the points on one side of each face and in the middle of each face
leftmid_idx <- c(1:9,19:26,35:44,50:52,56:59,62,63,64:71)
fiducial_pt_list_lm <- lapply(fiducial_pt_list, function(mat){return(mat[leftmid_idx,])})



#let's see the features of these chosen points on every face provided.
data<-feature(fiducial_pt_list_lm,c(train_idx,test_idx))

#emotion is a vector containing all the unique emotions on the faces provided
emotion<-unique(data$emotion_idx)



#1st: in the data dataset, calculate the mean of each feature in each emotion group, respectively;
#2nd: heihei is a data frame containing the mean of each feature in each emotion.
heihei<-c()
library(dplyr)
for (i in 1:length(emotion)) {
  datadata<-data %>%
    filter(emotion_idx==emotion[i]) %>%
    dplyr::select(-emotion_idx) %>%
    colMeans()
  h<-t(as.matrix(datadata))
  h<-cbind(h,emotion_idx=emotion[i])
  
  heihei<-rbind(heihei,h)
  heihei<-as.data.frame(heihei)
  
}

#-------------------------------------------------------------------
#Then, according to heihei data frame, if one feature changes very little in different emotions, 
#we can say that this feature is not useful in distinguishing emotions. So, we can delete features 
#that have small variation between emotions.

feature.var<-map_dbl(heihei[,-ncol(heihei)],function(x) var(x))


#the following, I choose certain variation level to delete features: <20% quantile

q<-quantile(feature.var,0.2)
new_index_20<-which(feature.var<q) #delete these features

#load("../output/new_index(1).RData")
dup_horiz <- new_index_20
dup_horiz=as.numeric(dup_horiz)
dup_horiz
```

```{r feature}
if(run.feature.train){

  source("../lib/feature.R")
  tm_feature_train <- NA
  dat_train <- feature(fiducial_pt_list_lm, train_idx)
  feature.construction.train.end = proc.time()
  
  dat_train <- dat_train[,-dup_horiz]
  feature.construction.train.end = proc.time()
  
  tm_feature_test <- NA
  dat_test <- feature(fiducial_pt_list_lm, test_idx)
  feature.construction.test.end = proc.time()
  
  dat_test <- dat_test[,-dup_horiz]
  feature.construction.test.end = proc.time()
  
  #time for training feature construction
  print(feature.construction.train.end - feature.construction.start)
  #time for test feature construction
  print(feature.construction.test.end - feature.construction.train.end)
  
  save(dat_train, file="../output/feature_train.RData")
  save(dat_test, file="../output/feature_test.RData")

}
```

###  load features

```{r}
load("../output/feature_train.RData")
load("../output/feature_test.RData")
```

###pca(linear method)
```{r}
n.pca.list <- c(30,50,75,120,150,200)
dim(dat_train)
dim(dat_test)
train.model.start = proc.time()

pca <- prcomp(dat_train[,-1514], cor=T)
pca
train_pca <- data.frame(pca$x[,1:50]) 
train_pca


pca2=predict(pca,dat_test[,-1514])
pca2
test_pca=data.frame(pca2[, 1:50])
test_pca


train_index<- dat_train[1514]
dat_train_pca <- cbind(train_pca, train_index)
dat_train_pca

test_index<- dat_test[1514]
dat_test_pca <- cbind(test_pca, test_index)
dat_test_pca

##training time 

lda.model_pca <- lda(emotion_idx ~ ., data=dat_train_pca) 
train.model.end = proc.time()
#time for training the model
print(train.model.end - train.model.start)
```

###pca(linear method)

```{r}
n.pca.list <- c(30,50,80,100,150,200)
dim(dat_train)
dim(dat_test)

train_time_pca=function(n.pca.list=n.pca.list)
for(i in 1:length(n.pca.list)){
  train.model.start = proc.time()
pca <- prcomp(dat_train[,-1514], cor=T)
train_pca <- data.frame(pca$x[,1:n.pca.list[i]]) 

pca2=predict(pca,dat_test[,-1514])
test_pca=data.frame(pca2[, 1:n.pca.list[i]])

train_index<- dat_train[1514]
dat_train_pca <- cbind(train_pca, train_index)

test_index<- dat_test[1514]
dat_test_pca <- cbind(test_pca, test_index)

##training time 

lda.model_pca <- lda(emotion_idx ~ ., data=dat_train_pca) 
#time for training the model
train.model.end = proc.time()

#time for testing the model
test.model.start = proc.time()
lda.test.pred_pca = predict(lda.model_pca, dat_test_pca[-dim(dat_test_pca)[2]])
test.model.end = proc.time()

#test accuracy
test_accuracy=confusionMatrix(lda.test.pred_pca$class, dat_test_pca$emotion_idx)$overall[1]

print(list(l1=train.model.end - train.model.start,
           l2=test.model.end - test.model.start,
           l3=test_accuracy))
}
train_time_pca(n.pca.list)
```

```{r}
#considering all the results including training time, testing time and accuracy,
#we choose 50 principle components.

train_pca_final <- data.frame(pca$x[,1:50]) 
dat_train_pca_final=cbind(train_pca_final, train_index)
pca2=predict(pca,dat_test[,-1514])
test_pca_final=data.frame(pca2[, 1:50])


dat_test_pca_final=cbind(test_pca_final, test_index)
dim(dat_test_pca_final)

save(dat_train_pca_final, file="../output/feature_pca_train.RData")
save(dat_test_pca_final, file="../output/feature_pca_test.RData")
```

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=FALSE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
run.feature.test.test=FALSE # process features for test_test set
```

### Tune the SVM model with cross-validation:
```{r}
tm_train=NA
tm_train <- system.time(tuned_parameters <- tune.svm(emotion_idx~., 
                                                     data = dat_train_pca_final, 
                                                     gamma = 10^(-5:-1), 
                                                     cost = c(30,35,40),
                                                     tunecontrol = tune.control(cross =12)
                                                     ))
summary(tuned_parameters)

```

```{r}
source("../lib/train_svm.R")
par_best=NULL
fit_train_final_svm <- train(dat_train_pca_final, tuned_parameters$best.parameters)
save(fit_train_final_svm, file="../output/fit_train_final.RData")


### Train accurancy:
source("../lib/test_svm.R")
load("../output/fit_train_final.RData")

if(run.test){
  pred_train <- test(fit_train_final_svm, dat_train_pca_final)
}
accu.train <- mean(dat_train_pca_final$emotion_idx == pred_train)
accu.train
# [1] 0.59

### SVM: Run test on test images
source("../lib/test_svm.R")
tm_test=NA
if(run.test){
  tm_test <- system.time(pred <- test(fit_train_final_svm, dat_test_pca_final))
}

### SVM: Run test_test on test images
source("../lib/test_svm.R")
tm_test=NA
if(run.test){
  load(file="../output/fit_train_final.RData")
  tm_test <- system.time(pred <- test(fit_train_final_svm, dat_test_pca_final))
}

### evaluation
accu <- mean(dat_test_pca_final$emotion_idx == pred)
cat("The accuracy of model:", "is", accu*100, "%.\n")
library(caret)
confusionMatrix(pred, dat_test_pca_final$emotion_idx)

### Summarize Running Time
### Prediction performance matters, 
### so does the running times for constructing features and for training the model, 
### especially when the computation resource is limited. 
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for testing model=", tm_test[1], "s \n")

```


#### 4. GBM (*Baseline Model*)
* Tune GBM.
```{r}
hyper_grid <- expand.grid(
  shrinkage = c(.001, .01),
  interaction.depth = c(1, 3),
  n.minobsinnode = c(5, 10),
  bag.fraction = c(.65, .8), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# randomize data
random_index <- sample(1:nrow(dat_train_pca_final), nrow(dat_train_pca_final))
random_train <- dat_train_pca_final[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  

  # train model
  gbm.tune <- gbm(
    formula =  emotion_idx~.,
    distribution = "multinomial",
    data = random_train,
    n.trees = 100,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}


hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r}
source("../lib/train_gbm.R")
tm_train=NA
tm_train <- system.time(fit_train_baseline <- train(dat_train_pca_final, par = NULL))
save(fit_train_baseline, file="../output/fit_train_baseline_final.RData")
### Train Error
source("../lib/test_gbm.R")
load("../output/fit_train_baseline_final.RData")

tm_test=NA
if(run.test){
  tm_test <- system.time(pred_train <- test(fit_train_baseline, dat_train_pca_final))
}
labels = colnames(pred_train)[apply(pred_train, 1, which.max)]
accu <- mean(dat_train_pca_final$emotion_idx == labels)
accu


### GBM: Run test on test images

source("../lib/test_gbm.R")
tm_test=NA
if(run.test){
  tm_test <- system.time(pred <- test(fit_train_baseline, dat_test_pca_final))
}

### GBM: Run test_test on test images
source("../lib/test_gbm.R")
tm_test_test=NA
if(run.feature.test.test){
  load(file="../output/fit_train_baseline_final.RData")
  tm_test <- system.time(pred <- test(fit_train_baseline, dat_test_selected))
}

### Evaluation
labels = colnames(pred)[apply(pred, 1, which.max)]
accu <- mean(dat_test_pca_final$emotion_idx == labels)
cat("The accuracy of model:", "is", accu*100, "%.\n")
library(caret)
confusionMatrix(as.factor(labels), dat_test_pca_final$emotion_idx)


### Summarize Running Time
### Prediction performance matters, 
### so does the running times for constructing features and for training the model,
### especially when the computation resource is limited. 

#cat("Time for constructing training features=", tm_feature_train[1], "s \n")
#cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for testing model=", tm_test[1], "s \n")
```
#ksvm training time 
```{r}
ksvmtrain.model.start = proc.time()
ksvm_model=ksvm(as.matrix(dat_train_pca_final[,1:50]),factor(dat_train_pca_final[,51]),scale = c(),
                type = "C-svc",kernel = 'vanilladot',C=10)
ksvmtrain.model.end = proc.time()
print(ksvmtrain.model.end-ksvmtrain.model.start)
```
#ksvm testing time 
```{r}
#training
pred.train<- predict(ksvm_model, newdata =dat_train_pca_final[,1:50])
accur.train= mean(pred.train == dat_train_pca_final$emotion_idx)
accur.train
#test
ksvmtest.model.start = proc.time()
pred1<- predict(ksvm_model, newdata =dat_test_pca_final[,1:50])
accur1= mean(pred1 == dat_test_pca_final$emotion_idx)
accur1
ksvmtest.model.end = proc.time()
print(ksvmtest.model.end-ksvmtest.model.start)
```

#xgboost training time 
```{r}
xgboosttrain.model.start = proc.time()
trainset1=as.matrix(dat_train_pca_final[,1:50])
trainset1=Matrix(trainset1,sparse=T) 
m.cv<-xgboost(data=trainset1,
                  label=as.numeric(dat_train_pca_final[,51])-1,
                  eta = 0.1,
                  max_depth = 15, 
                  nrounds=25, 
                  subsample = 0.5,
                  colsample_bytree = 0.5,
                  seed = 1,
                  #eval_metric = "merror",
                  objective = "multi:softmax",
                  num_class = 22,
                  nthread = 3)

xgboosttrain.model.end = proc.time()
print(xgboosttrain.model.end-xgboosttrain.model.start)

pred2<- predict(m.cv, newdata =as.matrix(dat_test_pca_final[,1:50]))+1
pred2
accur2= mean(pred2 == dat_test_pca_final$emotion_idx)
accur2
```



```{r}

# rename the label column
train_xgb=dat_train_pca_final
test_xgb=dat_test_pca_final
names(train_xgb)[51] <- "label"
names(test_xgb)[51] <- "label"
# factorize label column
train_xgb$label <- factor(train_xgb$label)
test_xgb$label <- factor(test_xgb$label)


# Here we use 10-fold cross-validation, repeating twice, and using random search for tuning hyper-parameters.
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats =2,  search = "random", allowParallel = TRUE)

# train a xgbTree model using caret::train
train_xgb_dat<-train_xgb[,1:50]

xgb_model <- train(x = train_xgb_dat, y = train_xgb$label,
                   trControl = fitControl,method = "xgbLinear",
                   verbose = TRUE)

print(xgb_model$results) # Model results

# Prediction
#y_pred_xgb <- predict(xgb_model, test_xgb)


# calculate accuracy
accu_xgb <- max(xgb_model$results$Accuracy)*100
cat("The accuracy of model on selected feature:", "is", round(accu_xgb,2), "%.\n")
# caret::confusionMatrix(y_pred_xgb, test_xgb$label)


train.model.end = proc.time()

#time for training the model
print(train.model.end - train.model.start)

```
## 3. Random Forest (this is a method we tried, but not the final improved method we picked)


```{r}
# Random forest model: 
set.seed(0)
traintask <- makeClassifTask(data = dat_train_pca_final,target = "emotion_idx") 
testtask <- makeClassifTask(data = dat_test_pca_final,target = "emotion_idx")
rf.lrn <- makeLearner("classif.randomForest")
rf.lrn$par.vals <- list(ntree = 100L, importance=TRUE)
rdesc <- makeResampleDesc("CV",iters=5L)
r <- resample(learner = rf.lrn, task = traintask, resampling = rdesc, measures = list(acc), show.info = T)
params <- makeParamSet(makeIntegerParam("mtry",lower = 10,upper = 50),makeIntegerParam("nodesize",lower = 10,upper = 50))
ctrl <- makeTuneControlRandom(maxit = 5L)
#tune parameters 
tune <- tuneParams(learner = rf.lrn, task = traintask, resampling = rdesc, measures = list(acc), par.set = params, control = ctrl, show.info = T) 
#the best is mtry=34; nodesize=19 acc.test.mean=0.4240000


model2 <- randomForest(as.factor(emotion_idx) ~ ., data = dat_train_pca_final, ntree = 100, mtry = 34, importance = TRUE)
model2

predTrain<-predict(model2, data=dat_train_pca_final,type="class")
sum(predTrain == dat_train_pca_final$emotion_idx)/length(dat_train_pca_final$emotion_idx)

predValid <- predict(model2, dat_test_pca_final, type = "class")
sum(predValid == dat_test_pca_final$emotion_idx)/length(dat_test_pca_final$emotion_idx)

rftime_train<- system.time(model2 <- randomForest(emotion_idx ~ ., data = dat_train_pca_final, ntree = 70, mtry = 36, importance = TRUE))
rftime_test<- system.time(predValid <- predict(model2, dat_test_pca_final, type = "class"))
rftime_train
rftime_test



```
#lda 
```{r}
source("../lib/train_lda.R")
tm_train=NA
tm_train <- system.time(fit_train_baseline <- train(dat_train_pca_final, par = NULL))
save(fit_train_baseline, file="../output/fit_train_baseline_final.RData")

### Train Error
source("../lib/test_lda.R")
load("../output/fit_train_baseline_final.RData")

tm_test=NA
if(run.test){
  tm_test <- system.time(pred_train <- test(fit_train_baseline, dat_train_pca_final))
}
accu <- mean(dat_train_pca_final$emotion_idx == pred_train$class)
accu
source("../lib/test_lda.R")
tm_test=NA
if(run.test){
  tm_test <- system.time(pred <- test(fit_train_baseline, dat_test_pca_final))
}

source("../lib/test_lda.R")
tm_test_test=NA
if(run.feature.test.test){
  load(file="../output/fit_train_baseline_final.RData")
  tm_test <- system.time(pred <- test(fit_train_baseline, dat_test_selected))
}

### Evaluation
accu <- mean(dat_test_pca_final$emotion_idx == pred$class)
cat("The accuracy of model:", "is", accu*100, "%.\n")
library(caret)
confusionMatrix(as.factor(labels), dat_test_pca_final$emotion_idx)
# ldatrain.model.start = proc.time()
# lda.model <- lda(emotion_idx ~ ., data=dat_train_pca) 
# ldatrain.model.end = proc.time()
# #time for training the model
# print(ldatrain.model.end - ldatrain.model.start)


### Summarize Running Time
### Prediction performance matters, 
### so does the running times for constructing features and for training the model,
### especially when the computation resource is limited. 

#cat("Time for constructing training features=", tm_feature_train[1], "s \n")
#cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for testing model=", tm_test[1], "s \n")
```